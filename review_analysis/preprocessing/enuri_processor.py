import pandas as pd
import re
import os
from datetime import datetime

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

from review_analysis.preprocessing.base_processor import BaseDataProcessor

class EnuriProcessor(BaseDataProcessor):
    """
    ì—ëˆ„ë¦¬ ë¦¬ë·° ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ ë° FEí•˜ëŠ” í´ë˜ìŠ¤ì´ë‹¤.
    ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì•Œê¸° ìœ„í•´ ì œê±° ì‚¬ìœ ë³„ í†µê³„ë¥¼ ì¶œë ¥í•œë‹¤.
    """

    def __init__(self, input_path: str, output_dir: str) -> None:
        super().__init__(input_path, output_dir)
        self.df: pd.DataFrame = pd.DataFrame()

    def _get_season(self, month: int) -> str:
        """ì›” ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³„ì ˆ íŒŒìƒë³€ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
        if 3 <= month <= 5:
            return 'Spring'
        elif 6 <= month <= 8:
            return 'Summer'
        elif 9 <= month <= 11:
            return 'Fall'
        else:
            return 'Winter'

    def preprocess(self) -> None:
        """
        [EDA ë° ì „ì²˜ë¦¬]
        í˜•ì‹ ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë°ì´í„°ë¥¼ í„°ë¯¸ë„ì— ì¶œë ¥í•˜ëŠ” ë¡œì§ì´ ì¶”ê°€ë˜ì—ˆë‹¤.
        """
        # 1. ë°ì´í„° ë¡œë“œ
        try:
            self.df = pd.read_csv(self.input_path)
        except Exception as e:
            print(f"[Error] íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return

        stats = {"ì´ˆê¸° ë°ì´í„° ê°œìˆ˜": len(self.df)}
        
        # 2. ê²°ì¸¡ì¹˜ ì œê±°
        pre_count = len(self.df)
        self.df.dropna(subset=['content', 'date', 'rating'], inplace=True)
        stats["ê²°ì¸¡ì¹˜(Null) ì œê±°"] = pre_count - len(self.df)
        
        # 3. ìë£Œí˜• ë³€í™˜ ë° í˜•ì‹ ì˜¤ë¥˜ ë°ì´í„° ì¶œë ¥
        pre_count = len(self.df)
        
        # ë³€í™˜ ì‹œë„ (ì˜¤ë¥˜ëŠ” NaT/NaNìœ¼ë¡œ ì²˜ë¦¬)
        temp_date = pd.to_datetime(self.df['date'], format='%Y.%m.%d', errors='coerce')
        temp_rating = pd.to_numeric(self.df['rating'], errors='coerce')
        
        # í˜•ì‹ ì˜¤ë¥˜ê°€ ë°œìƒí•œ í–‰ë“¤(ì›ë˜ nullì´ ì•„ë‹ˆì—ˆìœ¼ë‚˜ ë³€í™˜ í›„ nullì´ ëœ ê²½ìš°) ì¶”ì¶œ
        date_errors = self.df[temp_date.isna() & self.df['date'].notna()]
        rating_errors = self.df[temp_rating.isna() & self.df['rating'].notna()]
        
        # í„°ë¯¸ë„ì— ì˜¤ë¥˜ ë°ì´í„° ì¶œë ¥
        if not date_errors.empty:
            print("\n" + "!"*10 + " [ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜ ë°ì´í„°] " + "!"*10)
            print(date_errors[['date', 'content']].head()) # ë„ˆë¬´ ë§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒìœ„ ì¼ë¶€ë§Œ ì¶œë ¥
            print(f"ì´ {len(date_errors)}ê±´ì˜ ë‚ ì§œ ì˜¤ë¥˜ ë°œê²¬")
            
        if not rating_errors.empty:
            print("\n" + "!"*10 + " [ë³„ì  í˜•ì‹ ì˜¤ë¥˜ ë°ì´í„°] " + "!"*10)
            print(rating_errors[['rating', 'content']].head())
            print(f"ì´ {len(rating_errors)}ê±´ì˜ ë³„ì  ì˜¤ë¥˜ ë°œê²¬")

        # ì‹¤ì œ ë³€í™˜ ê²°ê³¼ ë°˜ì˜ ë° ê²°ì¸¡ì¹˜ ì œê±°
        self.df['date'] = temp_date
        self.df['rating'] = temp_rating
        self.df.dropna(subset=['date', 'rating'], inplace=True)
        stats["í˜•ì‹ ì˜¤ë¥˜(ë‚ ì§œ/ë³„ì ) ì œê±°"] = pre_count - len(self.df)

        # 4. ê¸°ê°„ ì´ìƒì¹˜ ì²˜ë¦¬ (10ë…„ ì „ ì´ìƒì˜ ê³¼ê±° ë°ì´í„°)
        pre_count = len(self.df)
        cutoff_date = pd.Timestamp.now() - pd.DateOffset(years=10)
        self.df = self.df[self.df['date'] > cutoff_date]
        stats["ê¸°ê°„ ì´ìƒì¹˜(10ë…„ ì „) ì œê±°"] = pre_count - len(self.df)
        
        # 5. ë³„ì  ì´ìƒì¹˜ ì²˜ë¦¬ (1~5ì  ë²”ìœ„ ë°–)
        pre_count = len(self.df)
        self.df = self.df[(self.df['rating'] >= 1) & (self.df['rating'] <= 5)]
        stats["ë³„ì  ì´ìƒì¹˜(ë²”ìœ„ ë°–) ì œê±°"] = pre_count - len(self.df)

        # 6. í…ìŠ¤íŠ¸ ì •ì œ ë° ê¸¸ì´ ì´ìƒì¹˜ ì²˜ë¦¬ (2ê¸€ì ì´í•˜)
        pre_count = len(self.df)
        self.df['cleaned_content'] = self.df['content'].apply(
            lambda x: re.sub(r'\s+', ' ', re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', 
                      str(x).replace("\n", " "))).strip()
        )
        self.df = self.df[self.df['cleaned_content'].str.len() > 2]
        stats["í…ìŠ¤íŠ¸ ê¸¸ì´ ì´ìƒì¹˜(2ì ì´í•˜) ì œê±°"] = pre_count - len(self.df)

        # 7. ì¤‘ë³µ ë¦¬ë·° ì œê±°
        pre_count = len(self.df)
        self.df.drop_duplicates(subset=['cleaned_content'], inplace=True)
        stats["ì¤‘ë³µ ë¦¬ë·° ì œê±°"] = pre_count - len(self.df)

        # ìµœì¢… ìš”ì•½ ì¶œë ¥
        print("\n" + "="*30)
        print(f" [{self.__class__.__name__} ì „ì²˜ë¦¬ ìš”ì•½] ")
        print("-"*30)
        for reason, count in stats.items():
            print(f" - {reason}: {count}ê°œ")
        print("-"*30)
        print(f" * ìµœì¢… ë‚¨ì€ ë°ì´í„°: {len(self.df)}ê°œ")
        print("="*30 + "\n")

    def feature_engineering(self) -> None:
        """
        [Feature Engineering ë‹¨ê³„]
        ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ ìƒì„±, í…ìŠ¤íŠ¸ í† í°í™”, ë¦¬ë·° ê¸¸ì´(review_length) ê³„ì‚°, ê·¸ë¦¬ê³  LDA í† í”½ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•œë‹¤.

        ì´ë•Œ í† í”½ ëª¨ë¸ë§ì€ CountVectorizerë¡œ ë²¡í„°í™”(BOW) ìˆ˜í–‰í•˜ì—¬ LDA ëª¨ë¸ì„ í†µí•´ ì ì¬ëœ 3ê°€ì§€ í† í”½ì„ ì¶”ì¶œí•œë‹¤.
        ë°ì´í„° ì €ì¥ ì‹œ ë‹¨ìˆœ ìˆ«ìê°€ ì•„ë‹Œ 'í† í”½ë²ˆí˜¸(í•µì‹¬í‚¤ì›Œë“œ)' í˜•íƒœë¡œ 'topic_id' ì»¬ëŸ¼ì„ ìƒì„±í•œë‹¤.
        (ì˜ˆ: 0(ë°°ì†¡_ë¹ ë¦„_ê¸°ì‚¬ë‹˜))
        """
        if self.df.empty: return
        print(" -> Feature Engineering ìˆ˜í–‰ ì¤‘...")

        # 1. ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜ 
        self.df['month'] = self.df['date'].dt.month
        self.df['season'] = self.df['month'].apply(self._get_season)

        # 2. í† í°í™”
        self.df['tokens'] = self.df['cleaned_content'].apply(lambda x: ' '.join(re.findall(r'[ê°€-í£a-zA-Z0-9]+', x)))
        self.df['review_length'] = self.df['cleaned_content'].apply(len)

        # ---------------------------------------------------------
        # LDA í† í”½ ëª¨ë¸ë§
        # ---------------------------------------------------------
        print(" -> ğŸ§  ë²¡í„°í™”(BOW) ë° í† í”½ ëª¨ë¸ë§(LDA) ìˆ˜í–‰ ì¤‘...")
        
        # (1) ë²¡í„°í™”
        vectorizer = CountVectorizer(max_features=1000, min_df=2)
        vectorized_data = vectorizer.fit_transform(self.df['tokens'])
        
        # (2) LDA ëª¨ë¸ë§
        lda_model = LatentDirichletAllocation(n_components=3, random_state=42)
        topic_output = lda_model.fit_transform(vectorized_data)
        
        # (3) í† í”½ ID ë° ë¼ë²¨ ìƒì„±
        topic_indices = topic_output.argmax(axis=1)
        feature_names = vectorizer.get_feature_names_out()
        topic_label_dict = {}
        
        print(" -> ğŸ·ï¸ í† í”½ ë¼ë²¨ ìƒì„± ì¤‘...")
        for topic_idx, topic in enumerate(lda_model.components_):
            top_features_ind = topic.argsort()[:-4:-1]
            top_words = [feature_names[i] for i in top_features_ind]
            
            # ë¼ë²¨ í¬ë§·: ex) 0(ë°°ì†¡_ë¹ ë¦„_ê¸°ì‚¬ë‹˜)
            keywords_str = "_".join(top_words)
            label = f"{topic_idx}({keywords_str})"
            topic_label_dict[topic_idx] = label
            print(f"    ğŸ“Œ Topic {topic_idx} -> {label}")
        
        self.df['topic_id'] = [topic_label_dict[idx] for idx in topic_indices]
        print(" -> âœ… 'topic_id' ì»¬ëŸ¼ ìƒì„± ì™„ë£Œ")


    def save_to_database(self) -> None:
        """ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•œë‹¤."""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

        save_path = os.path.join(self.output_dir, "preprocessed_reviews_Enuri.csv")
        self.df.to_csv(save_path, index=False, encoding='utf-8-sig')
        print(f"[Success] ì—ëˆ„ë¦¬ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {save_path}")